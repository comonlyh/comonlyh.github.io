---
layout:    post
title:     "速通EEGNxt"
author:    "Lyh"
header-style: text
catalog:   true
tags:
    - BCI
    
---

## 1.文章介绍
- 文章名:Toward reliable signals decoding for electroencephalogram: A benchmark study to EEGNeX
- 主要工作:对EEGNet模型进行改进,且有四处改进非常成功,使其成为媲美EEGNet的深度学习轻量级模型.
## 2.改进介绍
![alt text](image.png)
1. Block1:EEGNeX在其基础上加了一层(串联)Conv2d.
```python
self.block2 = nn.Sequential(
    nn.Conv2d(F, F*4, kernel_size=(1, 64), bias=False, padding='same'),
    nn.BatchNorm2d(F*4, momentum=0.01, affine=True, eps=1e-3)
)
#affine表示是否使用可学习的缩放和偏移参数
```
核心思想:渐进式特征学习,从基础时间模式到复杂时间模式.
2. 移除深度可分离卷积(因在block2中已经进行了深度卷积,深度可分离变得冗余),添加两个空洞卷积.
```python
# Block 4: 第一个空洞卷积
self.block4 = nn.Sequential(
    nn.Conv2d(F*4*D, F*4*D, kernel_size=(1, 16), bias=False, 
              padding='same', dilation=(1, 2)),  # dilation=2
    nn.BatchNorm2d(F*4*D, momentum=0.01, affine=True, eps=1e-3)
)

# Block 5: 第二个空洞卷积
self.block5 = nn.Sequential(
    nn.Conv2d(F*4*D, F, kernel_size=(1, 16), bias=False, 
              padding='same', dilation=(1, 4)),  # dilation=4
    nn.BatchNorm2d(F, momentum=0.01, affine=True, eps=1e-3),
    nn.ELU(),
    nn.AvgPool2d(kernel_size=(1, 4), stride=4),
    nn.Dropout(p=dropoutRate),
    nn.Flatten()
)
```
- 为什么说冗余?
- 在空间维度为1时,没有空间信息需要保持,1x1卷积只是做通道间的线性组合,这种通道融合可以用更简单的线性层实现.此时,空洞卷积通过扩大时间感受野来替代其功能.
3. 使用逆瓶颈结构->滤波器数量:8-32-64-64-8,先扩展后收缩.
4. 使用填充和减少激活层来增加模型感受野
## 3.插曲
起因:还未搞清什么原因,cursor上的聊天记录还没整理就丢了.
解决:"悟已往之不谏,知来者之可追",解决方法就是因聊天记录存储在SQLite数据库的state.vscdb文件中,使用网上推荐的DB Browser for SQLite工具打开文件(注意右下角选所有文件(.*),否则会被吞掉扩展名),然后开始找.
## 4.问题总结与解决方案
1. 2a数据集加载失败,数据格式不匹配.
- 解决方案:如下使用mne库处理gdf文件.
```python
import mne
import numpy as np
train_file = "file_path.gdf"  
# 1. 加载GDF文件
# preload=True表示将数据加载到内存，方便后续操作
raw_train = mne.io.read_raw_gdf(train_file, preload=True)  
# 2. 提取事件（标记数据中的事件点，如刺激出现的时刻等）
events_train = mne.find_events(raw_train)  
# 3. 定义epochs的时间窗口
# tmin：事件前的时间；tmax：事件后的时间
epochs_train = mne.Epochs(raw_train,
    events_train,
    tmin=-0.5,
    tmax=3.5,
    baseline=(None, 0),  # 基线校正，这里用事件前的所有时间作为基线
    preload=True)
# 4. 转换数据格式为[trials, channels, samples]
data = epochs_train.get_data()  
# 此时data的形状就是(试次数, 通道数, 每个试次的采样点数)
# 5. 标签映射 {7:0, 8:1, 9:2, 10:3}
# 获取原始事件标签
event_ids = epochs_train.events[:, 2]  
# 定义映射字典
label_mapping = {7: 0, 8: 1, 9: 2, 10: 3}  
# 进行标签映射
labels = np.array([label_mapping.get(event_id, -1) for event_id in event_ids])  
# 这里“-1”可作为未映射到的标签的默认值，实际需根据数据调整
```
2. 论文中的早停机制实现步骤?
- 解决方法:示例代码如下
```python
def train_single_subject(subject_id, n_epochs=500, early_stop_patience=30):
    # 早停机制变量初始化
    patience_counter = 0
    best_val_loss = float('inf')
    for epoch in range(1, n_epochs + 1):
        # 早停机制核心逻辑
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0  # 重置计数器
            # 保存最佳模型
            if val_acc > best_acc:
                best_acc = val_acc
                best_epoch = epoch
                torch.save({...}, model_save_path)
        else:
            patience_counter += 1  # 增加计数器
            if patience_counter >= early_stop_patience:
                print(f"早停触发，在第{epoch}个epoch停止训练")
                break  # 提前结束训练
```
- 总结原理如下:val_loss作为监控指标,patience_counter记录连续多少个epoch验证损失没有改善,early_stop_patience=30个验证损失都没有改善时,触发早停.
## 5.知识点补充
1. 早停机制:当验证准确率连续多轮不再提升甚至下降时,说明模型开始拟合训练数据,此时停止训练,保留泛化能力最好的模型状态.
2. Dilation参数(膨胀率)时卷积神经网络中用于扩大卷积核感受野的参数,无需增加计算量和参数量,能帮助模型更高效的捕捉全局信息.
   1. Dilation=1时,卷积核元素紧密排列,是常规卷积.
   2. Dilation=2时,卷积核元素间会插入一个空格,感受野范围翻倍.
3. 基线校正:神经信号预处理中的一个重要步骤,就是以信号里一段无任务期的静息值为参考,消除信号的直流偏移和基线偏移,让静息状态下的信号尽可能为0,让不同时段的信号有统一的基准.
## 6.实验结果简述
本人通过三个版本对论文进行了重现.感慨于早停机制对结果的影响之大.具体如下:
1. **版本总览**

| 版本  | 平均准确率 | 主要特点 |
|------|------------|----------|
| **版本1** | **89.25%** | 网络版本，使用CAR滤波,无早停 |
| **版本2**| **89.62%** | GitHub匹配版本，无早停 |
| **版本3**  | **78.65%** | 带早停和动态绘图 |

2. **差异汇总**

 *模型架构差异*

| 参数 | 版本1 | 版本2 | 版本3 |
|------|-------|-------|-------|
| **Block 1 kernel_size** | `(1, 32)` | `(1, 64)` | `(1, 64)` |
| **Block 2 kernel_size** | `(1, 32)` | `(1, 64)` | `(1, 64)` |
| **Block 4 kernel_size** | `(1, 8)` | `(1, 16)` | `(1, 16)` |
| **Block 5 kernel_size** | `(1, 8)` | `(1, 16)` | `(1, 16)` |
| **Block 5 dilation** | `(1, 2)` | `(1, 4)` | `(1, 4)` |
| **分类器max_norm** | ❌ 无 | ✅ 0.25 | ✅ 0.25| 

*训练参数差异*

| 参数 | 版本1 | 版本2 | 版本3 |
|------|-------|-------|-------|
| **epochs** | 500 | 500 | 早停控制 |
| **batch_size** | 32 | 128 | 128 |
| **learning_rate** | 0.0005 | 0.001 | 0.001 |
| **早停机制** | ❌ 无 | ❌ 无 | ✅ 有 (patience=30) |
| **动态绘图** | ❌ 无 | ❌ 无 | ✅ 有 |

*数据预处理差异*

| 步骤 | 版本1 | 版本2 | 版本3 |
|------|-------|-------|-------|
| **CAR滤波** | ✅ 有 | ❌ 无 | ❌ 无 |
| **数据分割比例** | 60% train, 20% val, 20% test | 75% train, 12.5% val, 12.5% test | 75% train, 12.5% val, 12.5% test |

3. **实验结果**
以被试7的相关结果为例:
![alt text](image-1.png)
可以看出早停对acc的影响很大(但准确率更贴合原论文),简单分析:
- 学习率调度过于激进,下降过快,可能导致模型在训练后期学习能力不足
- 此机制可能更适合图像等相对简答的数据,对EEG这种时序信号可能不够实用.
- 加上CAR滤波和带通滤波进行去噪可能会改善数据质量,提高acc.